{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer Overview\n",
    "This notebook is used to generate the trained model for the sentiment analyzer in the DiscordNLP bot. \n",
    "Note: The data sets to train this model **are not** included in the repository, so you will need to obtain them at https://www.kaggle.com/kazanova/sentiment140. The script `PrepareData.py` was used to remove unwanted columns and strip unwanted text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The following blocks handle library imports, and importing the data set as a Pandas dataframe. A vocabulary tokenizer will be generated and saved if one does not already exist. Once tokenized, the data will be padded to a fixed length and then split into training and validation data sets for the model.\n",
    "\n",
    "Note: The tokenizer is saved as a pickle, funniest shit I've seen. \n",
    "\n",
    "Second Note: Never load a pickle from a source you don't trust. They are serialized Python objects and can run all sorts of mean, nasty things on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"text\"]\n",
    "train_data = pandas.read_csv(\n",
    "    \"../data/training_data_long.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "data_clean = train_data[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../models/tokenizer.pickle\", \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "except:\n",
    "    tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "        data_clean, target_vocab_size=2**16\n",
    "    )\n",
    "    with open(\"../models/tokenizer.pickle\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = [tokenizer.encode(sentence) for sentence in data_clean]\n",
    "\n",
    "# pad sentences with 0's to match the longest sentence in the data set \n",
    "max_sentence_length = max([len(sentence) for sentence in data_input])\n",
    "data_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data_input, value=0, padding=\"post\", maxlen=max_sentence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = train_data[\"sentiment\"].to_numpy()\n",
    "test_idx = np.random.randint(0, math.floor(len(data_clean)/2), max(math.floor(len(data_clean)/200), 100))\n",
    "test_idx = np.concatenate((test_idx, test_idx+math.floor(len(data_clean)/2)))\n",
    "test_inputs = data_input[test_idx]\n",
    "test_labels = data_labels[test_idx]\n",
    "train_inputs = np.delete(data_input, test_idx, axis=0)\n",
    "train_labels = np.delete(data_labels, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Definition\n",
    "Next, several constants are declared to make tweaking the model easy. The model is defined sequentially with Keras, however, the convolution+maxpool layers are all in parallel with eachother. To do this with a sequential model, they are instantiated seperately, and then concatenated together to form one layer. This \"custom\" layer can then be appended to the sequential model like any other layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = len(set(train_data[\"sentiment\"]))\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=max_sentence_length)\n",
    "embedding = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_DIM)(input_layer)\n",
    "bigram = layers.Conv1D(filters=NB_FILTERS, \n",
    "                         kernel_size=2, \n",
    "                         padding=\"valid\", \n",
    "                         activation=\"relu\")(embedding)\n",
    "conv1 = layers.GlobalMaxPool1D()(bigram)\n",
    "\n",
    "trigram = layers.Conv1D(filters=NB_FILTERS, \n",
    "                         kernel_size=3, \n",
    "                         padding=\"valid\", \n",
    "                         activation=\"relu\")(embedding)\n",
    "conv2 = layers.GlobalMaxPool1D()(trigram)\n",
    "\n",
    "quadgram = layers.Conv1D(filters=NB_FILTERS, \n",
    "                         kernel_size=4, \n",
    "                         padding=\"valid\", \n",
    "                         activation=\"relu\")(embedding)\n",
    "conv3 = layers.GlobalMaxPool1D()(quadgram)\n",
    "\n",
    "concat = layers.Concatenate()([conv1, conv2, conv3])\n",
    "parallel_model = tf.keras.Model(input_layer, concat)\n",
    "\n",
    "Dcnn = tf.keras.models.Sequential()\n",
    "Dcnn.add(parallel_model)\n",
    "Dcnn.add(layers.Dense(units=NB_CLASSES, activation=\"softmax\"))\n",
    "Dcnn.add(layers.Dropout(rate=DROPOUT_RATE))\n",
    "if NB_CLASSES == 2:\n",
    "    Dcnn.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "else:\n",
    "    Dcnn.add(layers.Dense(units=NB_CLASSES, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../chkpts\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "Now that the model has been declared, we can shove some data through it. I don't have a working GPU, so this will likely go faster for you.\n",
    "\n",
    "After the model has been trained, its accuracy and loss are evaluated with the validation data set we made earlier. \n",
    "\n",
    "I also added some simple tests by hand just to make sure we get the outputs we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49503/49503 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.7247\n",
      "Epoch 00001: saving model to ../chkpts\n",
      "49503/49503 [==============================] - 7178s 145ms/step - loss: 0.4996 - accuracy: 0.7247\n",
      "Epoch 2/5\n",
      "49503/49503 [==============================] - ETA: 0s - loss: 0.4531 - accuracy: 0.7546\n",
      "Epoch 00002: saving model to ../chkpts\n",
      "49503/49503 [==============================] - 7179s 145ms/step - loss: 0.4531 - accuracy: 0.7546\n",
      "Epoch 3/5\n",
      "49503/49503 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.7759\n",
      "Epoch 00003: saving model to ../chkpts\n",
      "49503/49503 [==============================] - 7098s 143ms/step - loss: 0.4194 - accuracy: 0.7759\n",
      "Epoch 4/5\n",
      "49503/49503 [==============================] - ETA: 0s - loss: 0.3888 - accuracy: 0.7988\n",
      "Epoch 00004: saving model to ../chkpts\n",
      "49503/49503 [==============================] - 7109s 144ms/step - loss: 0.3888 - accuracy: 0.7988\n",
      "Epoch 5/5\n",
      "49503/49503 [==============================] - ETA: 0s - loss: 0.3634 - accuracy: 0.8145\n",
      "Epoch 00005: saving model to ../chkpts\n",
      "49503/49503 [==============================] - 7177s 145ms/step - loss: 0.3634 - accuracy: 0.8145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcbc68e17b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(train_inputs,\n",
    "         train_labels,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_manager.save()\n",
    "Dcnn.save(\"../models/sentiment_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 3s 6ms/step - loss: 0.4453 - accuracy: 0.8217\n",
      "[0.4453016519546509, 0.8216875195503235]\n",
      "[[0.8870019]]\n"
     ]
    }
   ],
   "source": [
    "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
    "print(results)\n",
    "text = \"You are so funny\"\n",
    "tokenized_input = np.array([tokenizer.encode(\"You are so funny\")])\n",
    "tokenized_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                  tokenized_input, value=0, padding=\"post\", maxlen=73\n",
    "                  )\n",
    "print(Dcnn(tokenized_input, training=False).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
